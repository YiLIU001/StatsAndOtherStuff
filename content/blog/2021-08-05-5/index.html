---
title: 高斯混合模型 5
author: Yi LIU
date: '2019-05-27'
slug: '5'
categories:
  - Work
tags:
  - Bayesian
  - Stats
  - Unsupervised learning
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
bibliography: [../Refs.bib]
biblio-style: "apalike"
link-citations: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#简介">简介</a></li>
<li><a href="#高斯混合模型的编程实现">高斯混合模型的编程实现</a></li>
<li><a href="#小结">小结</a></li>
</ul>
</div>

<p>本文是一系列文章中的一篇。其它文章如下：</p>
<ul>
<li><a href="/blog/2019-04-22-gmm-1">高斯混合模型1</a></li>
<li><a href="/blog/2019-04-27-gmm-2">高斯混合模型2</a></li>
<li><a href="/blog/3">高斯混合模型3</a></li>
<li><a href="/blog/4">高斯混合模型4</a></li>
<li><a href="/blog/6">高斯混合模型6</a></li>
<li><a href="/blog/7">高斯混合模型7</a></li>
</ul>
<div id="简介" class="section level1">
<h1>简介</h1>
<p>在前一篇文章里，我们引入了贝叶斯观点下的高斯混合模型，并且使用了Gibbs采样来近似参数的后验概率。但是之前的方法有一个问题，对于每个不同的模型，我们都需要根据这个模型来写出特定的采样方法。而且对于模型复杂的模型来说，这个特定的采样方法很难得到。那么有没有办法把这个复杂的步骤自动化呢？即我们只需要写出模型，通过一些方法来自动得到采样方法，并输出结果呢？这种方法是有的，我们可以通过概率编程语言（probabilstic programming language）来实现这一目的。</p>
<p>在本文中，我们把上一篇文章里的问题用概率编程语言来实现，作为这种方法的一个简单介绍。内容很简单，只是起一个引子的作用。</p>
</div>
<div id="高斯混合模型的编程实现" class="section level1">
<h1>高斯混合模型的编程实现</h1>
<p>我们先生成和之前一样的数据</p>
<pre class="r"><code>set.seed(1234)
x &lt;- c(rnorm(2000, -2), rnorm(2000, 2))
hist(x, breaks=500,freq = FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" />
下面我们载入<code>rjags</code>包，并定义模型</p>
<pre class="r"><code>suppressMessages(library(rjags))
model_string &lt;- &quot;
model {
# likelihood
for (i in 1:N) {
z[i] ~ dcat(pi[1:2])
x[i] ~ dnorm(mu[z[i]], 1)
}

# priors
pi ~ ddirch(c(1,1))
mu ~ dmnorm(c(0,0), W)

}
&quot;</code></pre>
<p>现在我们来进行Gibbs采样</p>
<pre class="r"><code>j1 &lt;- jags.model(
  file=textConnection(model_string),
  data=list(x=x,
            N=length(x),
            W=diag(0.2, 2, 2)),
  n.adapt = 100)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 4000
##    Unobserved stochastic nodes: 4002
##    Total graph size: 12014
## 
## Initializing model</code></pre>
<pre class="r"><code>j1_samples &lt;- jags.samples(j1, c(&quot;pi&quot;, &quot;mu&quot;), n.iter=1000)</code></pre>
<p>下面我们看看<code>rjags</code>程序给出的结果</p>
<pre class="r"><code>mu_samples &lt;- as.mcmc.list(j1_samples$mu)

plot(mu_samples[,1])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>plot(mu_samples[,2])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-2.png" width="672" />
从上面的图可以看出，程序的结果不错。</p>
</div>
<div id="小结" class="section level1">
<h1>小结</h1>
<p>之前我们实现Gibbs采样时，具体采样方法需要自己推导和实现。这样的做法很容易产生错误，而且工作量也比较大。在本文中们，我们介绍了一个可以使这一系列工作系统化的包<code>rjags</code>。和之前的代码比起来，本文中的代码要简单很多，而且结果也很好。</p>
<p>实际上，<code>rjags</code>是一种概率编程语言。类似的包还有很多，比如：</p>
<ul>
<li><a href="https://mc-stan.org/"><code>stan</code></a>：有多种程序语言接口，包括<code>R</code>，<code>python</code>，<code>Julia</code>等。<code>stan</code>主要支持Hamiltonian Monte Carlo，因此并不支持对于离散变量的采样。我们介绍的高斯混合模型中的<span class="math inline">\(z\)</span>就无法通过<code>stan</code>来获得。但是对于高斯混合模型，可以直接使用不含隐变量的对数似然函数来建模求解（即marginalization trick）。</li>
<li><a href="https://docs.pymc.io/"><code>pymc3</code></a>：使用<code>theano</code>做后端的<code>python</code>包，可以使用MCMC来近似后验概率，也可以使用变分推断来近似后验概率（关于变分近似，请看后续文章）。目前<code>pymc4</code>也在开发，据说会使用<code>tensorflow</code>做后端。</li>
<li><a href="https://turing.ml/"><code>turing</code></a>：一个<code>Julia</code>包，主要是MCMC采样。我没有太多经验，就不多做介绍了。</li>
</ul>
<p>它们的功能要远比实现一个高斯混合模型复杂的多。这里我们只是稍做介绍，关于具体的功能，可以参看它们的主页。</p>
</div>
