---
title: 高斯混合模型 1
author: Yi LIU
date: '2019-04-22'
slug: []
categories:
  - Work
tags:
  - Stats
  - Bayesian
description: First article on Gaussian Mixture Model
---

# 简介
机器学习可以分成三个子领域：监督学习，无监督学习和强化学习。无监督学习可以看成是“没有老师情况下的学习”，因为只有数据本身，没有监督学习里的标签，也没有强化学习里的反馈。

这里我们介绍一种聚类方法，高斯混合模型(Gaussian mixture model)。（这里要和mixed model区分开来，mixed model是一种统计模型，主要用来处理重复测量，或者有群组效应的数据，可以认为是一种监督学习方法）。高斯混合模型是一种基于模型的聚类方法，它是混合模型的一种特例，因为它只考虑高斯分布的混合。它和另一种常见的聚类方法K-Means有很多相似性。从某种程度上来说，K-Means是高斯混合模型的一种特殊情况。能使用K-Means的问题就能使用高斯混合模型，比如图像压缩中使用的矢量量化（vector quantization，参见Elements of statistical learning, Chap 14）。除此之外，高斯混合模型还可以用来做异常检测。


# 模型定义
具体模型如下：我们有一批数据$\{x_i\}, i=1,\dots, n$,其中每个数据都是独立服从一个混合高斯分布。即
$$x_i \overset{\text{i.i.d}}{\sim} f(x_i)  $$

其中的概率密度函数可写成
$$f(x) = \sum_{k=1}^K \pi_k \phi_k(x) $$

这里的$\phi_k$是一个高斯分布，有自己的参数$\mu_k$和 $\sigma^2_k$。同时，$x_i$来自每个高斯分布的概率为$\pi_k$，满足
$$\pi_k \geq 0 \text{, } \sum_k \pi_k = 1 $$

模型的参数有
$$\pi_k, k=1, \dots, K, $$
满足
$$\pi_k \geq 0, \sum_k \pi_k = 1 $$
同时还有高斯分布的参数
$$ \mu_k, \sigma^2_k, k=1, \dots, K $$
此外还有一个超参数$K$。这里我们给出的记号是一维数据，但是推广到高维空间也并不难，只需要
$$\mu_k \rightarrow \vec{\mu}_k$$
$$\sigma_k^2 \rightarrow \Sigma_k, \Sigma_k \text{ semi-definite}$$
其中，$\vec{\mu}_k$是$p$维空间向量，$\Sigma_k$是$p\times p$半正定矩阵。在后续的论述中，我们会先使用一维数据为例子，然后再推广到高维空间。

因此，对于数据，我们可以写出对数似然函数
$$ \ell(\theta|x) = \sum_{i=1}^n \log \left( \sum_k \pi_k \phi_k(x_i) \right)$$

# 为什么不用梯度下降法求解？
对于参数估计，我们使用常用的最大似然估计法（MLE）。但是对于高斯混合模型的求解一般不会用梯度下降（或者上升）法来求解。这里有几个原因：

1. 上一节中的对数似然函数不是一个凹函数，有多个极值点。但是这一点不是特别重要，其它方法也会面临同样的问题。我们可以多选取几个初始值，然后选那个最优解。这样至少可以得到不错的估计。
2. 同时，这个问题有轮换对称性(permutation symmetry)：对于参数估计进行轮换变换$\{1, \dots, K\} \rightarrow \{perm(1),\dots, perm(K)\}$，得到同样的似然函数值。
2. 由于这个问题有一些约束，比如$\pi_k \geq 0 \text{, } \sum_k \pi_k = 1$。还有比如$\sigma^2_k > 0$，在一维数据上这不是一个问题，但是在高维空间里，我们需要$\Sigma_k$是半正定(semi-definite)矩阵。这会让优化求解比较复杂，需要引入拉格朗日乘子法，或者重新定义参数。
2. 另一个问题是梯度下降对于高斯混合模型的求解不是一个效率很高的方法。这一点我们在下面用一个简化的问题来说明。

我们利用一个混合高斯模型生成数据，这里真实的参数是$\pi_1=\pi_2=0.5$, $\mu_1=-2$, $\mu_2=2$, $\sigma^2_1=\sigma^2_2=1$
```{r, fig.width=5}
set.seed(1234)
x <- c(rnorm(2000, -2), rnorm(2000, 2))
hist(x, breaks=500,freq = FALSE)
```

为了简化这个参数估计，我们只估计两个高斯分布的均值$\mu_1, \mu_2$。那么这个问题的对数似然函数就是
$$\ell(\mu_1, \mu_2|x) = \sum_{i=1}^n \log\left( \pi_1 \phi(x_i;\mu_1, \sigma^2_1) + \pi_2 \phi(x_i;\mu_2, \sigma^2_2)\right)  $$
写成函数有
```{r}
pi1 <- 0.5
pi2 <- 0.5
sd1 <- 1
sd2 <- 1
loglik <- function(mus, xs) {
  n_obs <- length(xs)
  loglik <- 0
  for (i in 1:n_obs) {
    loglik =loglik + log(pi1 * dnorm(xs[i], mus[1], sd1) + pi2*dnorm(xs[i], mus[2], sd2))
  }
  loglik
}
```
但是这个函数是一个为了突出算法比较慢的写法。在`R`语言中，矢量化的算法一般会比for循环要高效，因为通常矢量化算法可以调用语言底层优化过的函数，这些底层函数通常是`Fortran`或者`C`写的，效率比较高。因此我们可以写出一个高效一点的函数
```{r}
loglik_vec <- function(mus, xs) {
  prob1 <- pi1*dnorm(xs, mus[1], sd1)
  prob2 <- pi2*dnorm(xs, mus[2], sd2)
  loglik <- sum(log(prob1 + prob2))
}
```
比较一下两种写法的结果
```{r}
res1 <- loglik(c(-2,2), x)
res2 <- loglik_vec(c(-2, 2), x)
print(paste("Simple imp result:", res1))
print(paste("Vectorized imp result:", res2))
```

我们可以比较一下两个函数的速度
```{r}
library(microbenchmark)
microbenchmark(loglik(c(-2,2), x),
               loglik_vec(c(-2,2), x))
```
所以我们使用第二种矢量化的函数。

下面我们看看这个对数似然函数的样貌(landscape)。
```{r, echo=FALSE}
suppressMessages(library(tidyverse))
suppressMessages(library(purrr))
mus <- seq(-5, 5, by=0.1)
loglik_df <- tibble(mu1=mus, mu2 = mus)
loglik_df <- loglik_df %>% expand(mu1, mu2)
loglik_val <- map2(loglik_df$mu1, loglik_df$mu2, c) %>% map(loglik_vec, x) %>% unlist()
loglik_df$loglik <- loglik_val
ggplot(loglik_df, aes(x=mu1, y=mu2)) +
  geom_raster(aes(fill=loglik)) +
  geom_contour(aes(z=loglik), bins=30) + geom_abline(intercept = 0, slope = 1, color="red", linetype="dashed") +
  theme_bw() +
  annotate("text", x=c(2, -2), y=c(-2,2), label="x")
```
很明显的一点是对数似然函数是对称的，即$\mu_1 \leftrightarrow \mu_2$，我们得到同样的对数似然函数。这一点在前面介绍过。随着$K$的增大，这种对称性也会变强，导致也要有更多的似然函数的极值点。

下面我们来比较一下梯度下降法和EM算法的效率。对于梯度下降法，对数似然函数的梯度是
$$\frac{\partial \ell}{\partial \mu_1} = \sum_{i=1}^n \frac{\pi_1 \phi_1(x_i) (-) (\mu_1-x_i)/\sigma^2}{\pi_1 \phi_1(x_i) + \pi_2\phi_2(x_i)}$$
$$\frac{\partial \ell}{\partial \mu_2} = \sum_{i=1}^n \frac{\pi_2 \phi_2(x_i) (-) (\mu_2-x_i)/\sigma^2}{\pi_1 \phi_1(x_i) + \pi_2\phi_2(x_i)}$$
现在我们可以定义梯度计算的函数
```{r}
dmu1 <- function(mu1, mu2) {
  sum(-(mu1-x)*pi1*dnorm(x,mu1, sd1)/(sd1^2*(pi1*dnorm(x, mu1, sd1) +pi2*dnorm(x, mu2, sd2))))
}
dmu2 <- function(mu1, mu2) {
  sum(-(mu2-x)*pi2*dnorm(x,mu2, sd2)/(sd2^2*(pi1*dnorm(x, mu1, sd1) +pi2*dnorm(x, mu2, sd2))))
}
```
下面是用梯度上升法估计参数的结果。其中的学习率在一个范围内选取了一个最合适的值。
```{r, echo=FALSE,warning=FALSE}
mu1 <- 0.5
mu2 <- -0.5
lr <- 1e-4
tot_steps <- 100
log_like_gradDesc <- numeric(tot_steps)

mu1_tra <- numeric(tot_steps)
mu2_tra <- numeric(tot_steps)
for (step_i in 1:100) {
  d1 <- dmu1(mu1, mu2)
  d2 <- dmu2(mu1, mu2)
  mu1_new <- mu1 + lr*d1
  mu2_new <- mu2 + lr*d2
  log_like_gradDesc[step_i] <- loglik_vec(c(mu1, mu2), x)
  mu1 <- mu1_tra[step_i] <- mu1_new
  mu2 <- mu2_tra[step_i] <- mu2_new
}

plot(1:tot_steps, log_like_gradDesc, xlab = "step", ylab="Log-likelihood")

mu1_tra <- c(0.5, mu1_tra)
mu2_tra <- c(-0.5, mu2_tra)
mu_tra <- tibble(mu1=mu1_tra,
                 mu2=mu2_tra)
mu_tra <- mu_tra %>% 
  rename(mu1_start=mu1,
         mu2_start=mu2) %>% 
  mutate(mu1_end=lead(mu1_start),
         mu2_end=lead(mu2_start)) %>% 
  filter(!is.na(mu1_end))

loglik_df %>% filter(mu1 >= 0, mu1 <=5, mu2 >= -5, mu2 <= 0) %>%
  ggplot(aes(x=mu1, y=mu2)) +
  geom_raster(aes(fill=loglik)) +
  geom_contour(aes(z=loglik), bins=30) +
  theme_bw() +
  annotate("text", x=c(2, -2), y=c(-2,2), label="x") +
  geom_segment(data=mu_tra, aes(x=mu1_start, y=mu2_start, xend=mu1_end, yend=mu2_end), arrow = arrow(length = unit(0.01, "npc"))) +
  xlim(0, 5) +
  ylim(-5, 0)
```

现在我们用EM算法来求解，算法细节在后面的文章里给出，这里直接给出结果。
```{r, echo=FALSE, warning=FALSE}
cal_z <- function(mu1, mu2) {
  
  z <- matrix(NA_real_,
              nrow=length(x),
              ncol=2)
  
  for (i in 1:length(x)) {
    z[i,] <- c(
      dnorm(x[i], mean=mu1, sd=sd1),
      dnorm(x[i], mean=mu2, sd=sd2))
  }
  z/rowSums(z)
}

update_mu <- function(z) {
  zi_x <- z * x
  colSums(zi_x) / colSums(z)
}

log_lik_EM <- numeric(tot_steps)
mu1_EM <- numeric(tot_steps)
mu2_EM <- numeric(tot_steps)

mu1 <- 0.5
mu2 <- -0.5

for (step_i in 1:tot_steps) {
  z <- cal_z(mu1, mu2)
  res1 <- update_mu(z)
  mu1 <- mu1_EM[step_i] <- res1[1]
  mu2 <- mu2_EM[step_i] <- res1[2]
  log_lik_EM[step_i] <- loglik_vec(c(mu1, mu2), x)
}

mu1_EM <- c(0.5, mu1_EM)
mu2_EM <- c(-0.5, mu2_EM)
mu_EM <- tibble(mu1=mu1_EM,
                 mu2=mu2_EM)
mu_EM <- mu_EM %>% 
  rename(mu1_start=mu1,
         mu2_start=mu2) %>% 
  mutate(mu1_end=lead(mu1_start),
         mu2_end=lead(mu2_start)) %>% 
  filter(!is.na(mu1_end))

plot(1:tot_steps, log_lik_EM, xlab = "step", ylab="Log-likelihood")

loglik_df %>% filter(mu1 >= 0, mu1 <=5, mu2 >= -5, mu2 <= 0) %>%
  ggplot(aes(x=mu1, y=mu2)) +
  geom_raster(aes(fill=loglik))+
  geom_contour(aes(z=loglik), bins=30) +
  theme_bw() +
  annotate("text", x=c(2, -2), y=c(-2,2), label="x") +
  geom_segment(data=mu_EM, aes(x=mu1_start, y=mu2_start, xend=mu1_end, yend=mu2_end), arrow = arrow(length = unit(0.01, "npc"))) +
  xlim(0, 5) +
  ylim(-5,0)
```

从上面的图可以看出，在第二步的时候EM算法就收敛了，而梯度优化法则在约15步才收敛。因此对于高斯混合模型来说，EM算法是更有效的求解方法。在下一篇文章里，我们会详细介绍EM算法。

# 小结
在本篇文章里，我们引入了一种主要用于聚类的无监督学习模型：高斯混合模型。这种模型可以看成是混合模型的一种特例，而常用的聚类模型K-Means可以看成是高斯混合模型的一种特例。

通过一个简化的模型，我们说明了对于混合模型，梯度上升（或下降）法并不是一个很高效的优化求解算法。在混合模型中，常用的优化算法是EM算法。
