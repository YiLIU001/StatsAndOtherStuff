---
title: 高斯混合模型 4
author: Yi LIU
date: '2019-05-19'
slug: '4'
categories:
  - Work
tags:
  - Bayesian
  - Stats
  - Unsupervised learning
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
bibliography: [../Refs.bib]
biblio-style: "apalike"
link-citations: true
---

# 简介
在前面几篇文章里面，我们介绍了高斯混合模型的定义，它的求解方法（EM算法），以及EM算法的数学背景以及更广义的MM算法。在本篇文章里，我们换一个角度来看高斯混合模型，从贝叶斯角度来分析它。但是因为本文只是阐明一种方法，并不是介绍十分详尽的方法，所以考虑一个简化的问题：假设方差$\{\sigma^2_k\}$已知。

# 高斯混合模型的生成模型
在前面我们在介绍高斯混合模型的求解方法EM算法时，引入了隐变量$z$。当时是说它是每个观察值的组分指示器
$$z_{ik} = \left\{\begin{split} &1,& \text{ if }x_i \in k\text{ th-comp} \\ &0, & \text{ otherwise} \end{split}\right.$$

我们也可以从一个生成模型的角度来看待这个隐变量以及高斯混合模型的观察值，对于观察值$x_i$，它的产生过程是：

1. 产生组分指示器$z_{i} \sim Categorical(\pi)$
2. 根据$z_{i}$的值来生成观察值$x_i \sim \mathcal{N}(\mu_j, \sigma^2_{j})$，其中$z_{ij}=1$

这里的$\pi$就是个组分比例。用图形来表示的话，就是
```{r, echo=FALSE}
DiagrammeR::grViz("
digraph generative_model {
  rankdir = ub
  subgraph cluster1 {
  graph [overlay = true, fontsize = 10]
  node [shape = box] x
  
  node [shape = box, color = red, label='z', style = dashed] z

  z->x
  }
  node [shape = box, label='&pi;'] pi

  node [shape = box, label='&mu;'] mu
  mu->x
  pi->z
}
")
```
其中实心方框里的是观察值，隐变量$z$用红色虚线方框标出。

# 贝叶斯方法求解高斯混合模型
贝叶斯模型的大致步骤是：

1. 对于参数，我们有先验分布（prio distribution）$p(\theta)$
2. 对于观察到的数据，我们可以得到似然函数$p(x|\theta)$
3. 最终目的是得到参数的后验分布（posterior distribution）$p(\theta|x)  = p(\theta)p(x|\theta) / p(x)$

但是因为一般后验分布中的分母$p(x) = \int p(\theta)p(x|\theta) dx$很难得到解析解，我们一般都需要通过近似方法来得到后验概率。一种常见的数值近似方法是使用Markov chain Monte Carlo方法来得到后验分布的近似分布。

对于含有隐变量的问题来说，我们就有
$$p(\theta|x) = \int p(\theta, z|x) dz = \int dz \frac{p(\theta) p(z|\theta)p(x|z,\theta)}{p(x)}$$

具体对于高斯混合模型来说，问题的联合概率可以写出来
$$p(\pi, \mu, z, x) = p(\pi) p(\mu)p(z|\pi)p(x|z,\mu)$$
要求解的后验概率是
$$p(\pi, \mu| x) = \int p(\pi, \mu, z| x) dz$$
下面我们先来求包含隐变量的后验概率分布
$$ p(\pi, \mu, z| x) $$

如同前面提到的，对于高斯混合模型的后验分布，我们也很难解析求解，必须使用近似方法求解。这里我们使用Gibbs sampling方法来求解。这个方法的大意是在$t$迭代中，我们利用之前一次迭代的参数值和条件概率来更新参数值
$$\theta^{(t)}_j\sim p(\theta_j|\theta_{-j}^{(t-1)}, x)$$
其中
$$\theta_{-j}^{(t-1)}=(\theta_i^{(t)}, \dots, \theta_{j-1}^{(t)}, \theta_{j+1}^{(t-1)}, \dots , \theta_{p}^{(t-1)})$$

那么对于高斯混合模型来说，我们的策略是，在$t$次迭代中

1. 生成隐变量
$$z_{ik}^{(t)}\sim p(z_{ik}|\theta_{-z_{ik}}^{(t-1)}, x) \propto p(z_{ik}|\pi) p(x|z_{ik}, \mu_k^{(t-1)}) = \prod_i \pi_k^{(t-1)} \phi(x_i; \mu_k^{(t-1)}, \sigma_k^2)$$
2. 生成组分比例参数
$$\pi^{(t)}\sim p(\pi|\theta_{-\pi}^{(t-1)}, x) \propto p(\pi) p(z^{(t)}|\pi)$$
这里我们选取狄利克雷（Dirichlet）先验分布$p(\pi)=\mathcal{D}(\gamma_1, \dots, \gamma_K)$，那么后验分布就是$p(\pi)p(z^{(t)}|\pi) = \mathcal{D}(\gamma_1+n_1, \dots, \gamma_K+n_K)$，其中$n_k=\sum_i z_{ik}$
3. 生成均值
$$\mu_k^{(t)}\sim p(x|z^{(t)}, \mu, \sigma^2)p(\mu)$$
如果选取$p(\mu_k)=\mathcal{N}(\nu, \tau^2)$，那么我们有后验分布$\mu_k \sim \mathcal{N}(\nu_k(z^{(t)}), \tau_k^2(z^{(t)}))$，其中
$$\nu_k(z^{(t)})=\frac{\nu \sigma^2 + n_k\bar{x}_k \tau^2}{\sigma^2 + n_k \tau^2}$$
$$\tau_k^2(z^{(t)}) = \frac{\sigma^2 \tau^2}{\sigma^2 + n_k \tau^2}$$
$$n_k = \sum_{i}  z_{ik}^{(t)} $$
$$\bar{x}_k = \sum_{i, z_{ik}^{(t)}=1} x_i/n_k$$
关于高斯分布参数更新的具体推导参见[An Introduction to Bayesian Thinking第二章](https://statswithr.github.io/book/bayesian-inference.html#three-conjugate-families)

# 代码实现
```{r}
set.seed(1234)
x <- c(rnorm(2000, -2), rnorm(2000, 2))
hist(x, breaks=500,freq = FALSE)
```

```{r}
tot_steps <- 1000
z <- matrix(rep(0, 8000), ncol=2)
pi_mat <- matrix(rep(0, 2*tot_steps), ncol=2)


mu1_vec <- rep(0, tot_steps)
mu2_vec <- rep(0, tot_steps)

library(extraDistr)
draw_z1 <- function(mu1, mu2, pi1, pi2) {
  z1 <- numeric(length(x))
  for (i in 1:length(x)) {
    prob1 <- pi1 * dnorm(x[i], mean=mu1)
    prob2 <- pi2 * dnorm(x[i], mean=mu2)
    prob1 <- prob1 / (prob1 + prob2)
    z1[i] <- rbinom(1, 1, prob=prob1)
  }
  z1
}

gamma1 <- 1
gamma2 <- 1
draw_pi <- function() {
  pi <- rdirichlet(1, c(gamma1+n1, gamma2+n2))
  pi
}

nu <- 0
sigma1 <- 1
sigma2 <- 1
tau <- 5

draw_mu <- function() {
  nu1 <- (nu*sigma1^2 + sum(x[z[,1]==1]) * tau^2) / (sigma1^2 + n1*tau^2)
  tau1 <- sigma1*tau/sqrt(sigma1^2 + n1*tau^2)
  mu1 <- rnorm(1, mean=nu1, sd=tau1)
  
  nu2 <- (nu*sigma2^2 + sum(x[z[,2]==1]) * tau^2)/(sigma2^2 + n2*tau^2)
  tau2 <- sigma2*tau/sqrt(sigma2^2 + n2*tau^2)
  mu2 <- rnorm(1, mean=nu2, sd=tau2)
  
  c(mu1, mu2)
}

mu1 <- 0.5
mu2 <- -0.5

pi1 <- 0.5
pi2 <- 0.5

mu_vec <- c(mu1, mu2)
pi_update <- c(pi1, pi2)
for (i in 1:tot_steps) {
  # update z
  z[,1] <- draw_z1(mu_vec[1], mu_vec[2], pi_update[1], pi_update[2])
  z[,2] <- 1 - z[,1]
  
  n1 <- sum(z[,1])
  n2 <- sum(z[,2])
  
  # update pi
  pi_update <- draw_pi()
  pi_mat[i,] <- pi_update
  
  # update mu
  mu_vec <- draw_mu()
  mu1_vec[i] <- mu_vec[1]
  mu2_vec[i] <- mu_vec[2]
  
}
```

```{r}
mean(mu1_vec)
par(mfrow=c(1, 2))
hist(mu1_vec, breaks = 30)
plot(mu1_vec)
```

```{r}
mean(mu2_vec)
par(mfrow=c(1, 2))
hist(mu2_vec, breaks = 30)
plot(mu2_vec)
```

我们还可以看看结果的分布
```{r, echo=FALSE}
loglik_vec <- function(mus, xs) {
  prob1 <- pi1*dnorm(xs, mus[1], sigma1)
  prob2 <- pi2*dnorm(xs, mus[2], sigma2)
  loglik <- sum(log(prob1 + prob2))
}
```
```{r, echo=FALSE}
suppressMessages(library(tidyverse))
suppressMessages(library(purrr))
mus <- seq(-5, 5, by=0.1)
loglik_df <- tibble(mu1=mus, mu2 = mus)
loglik_df <- loglik_df %>% expand(mu1, mu2)
loglik_val <- map2(loglik_df$mu1, loglik_df$mu2, c) %>% map(loglik_vec, x) %>% unlist()
loglik_df$loglik <- loglik_val

mu_df <- tibble(mu1=c(0.5, mu1_vec),
                mu2=c(-0.5, mu2_vec))
mu_df <- mu_df %>% 
  mutate(mu1_start=mu1,
         mu2_start=mu2,
         mu1_end=lead(mu1_start),
         mu2_end=lead(mu2_start)) %>% 
  filter(!is.na(mu1_end))

loglik_df %>% filter(between(mu1, 0, 5), between(mu2, -5, 0)) %>% 
  ggplot(aes(x=mu1, y=mu2)) +
  geom_raster(aes(fill=loglik)) +
  geom_contour(aes(z=loglik), bins=30) +
  theme_bw() +
  geom_segment(data=mu_df, aes(x=mu1_start, y=mu2_start, xend=mu1_end, yend=mu2_end), arrow = arrow(length = unit(0.01, "npc")))
```

从上面的结果可以看到，我们的算法效果还不错。

# 小结
在本文中，我们介绍了贝叶斯观点下的高斯混合模型，并且使用MCM的一个特例，Gibbs采样来近似了数据的后验概率。与之前的文章类似，本文的目的是通过一些代码，尽可能简单的阐明一些算法实现，对于算法的质量并没有太多要求。
