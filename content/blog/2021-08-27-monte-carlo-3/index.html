---
title: Monte Carlo方法3
author: Yi LIU
date: '2021-08-27'
slug: []
categories:
  - Work
tags:
  - Bayesian
  - Stats
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
bibliography: [../Refs.bib]
biblio-style: "apalike"
link-citations: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#mala">MALA</a></li>
<li><a href="#implementation">Implementation</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>在上一篇的文章里，我们大致解释了一下Markov chain Monte Carlo (MCMC)：使用一个<code>proposal distribution</code><span class="math inline">\(q(x|x&#39;)\)</span>作为转移概率生成一个稳态分布是目标分布的<span class="math inline">\(p(x)\)</span>的Markov chain。在这个Markov chain上采样，达到稳态后的样本就可以认为是从目标分布采样的结果。之前我们使用的是一种简单的方法，Random Walk Metropolis-Hasting(RWMH)算法，这个算法十分简单，不依赖于目标分布，因此通用性也很强（也就是说同样的<span class="math inline">\(q(x|x&#39;)\)</span>可以用来生成不同的<span class="math inline">\(p(x)\)</span>的采样）。但是这个通用性也带来一个问题，就是采样效率（收敛到稳态分布的速度）可能没有那么高。 那么一个想法就是使用<span class="math inline">\(p(x)\)</span>的信息来提高采样效率。</p>
<p>加速采样效率的一个自然的想法是利用目标分布的信息来提高Markov chain在状态空间里的探索速度。比较简单的局部信息有梯度，下面我们介绍一种使用梯度来加速探索速度的算法Metropolis Adjusted Langevin Algorithm (MALA)。这种算法利用了Langevin diffusion的稳态分布。</p>
</div>
<div id="mala" class="section level1">
<h1>MALA</h1>
<p>MALA的大致思想如下：如果<span class="math inline">\(x\)</span>满足下面的扩散方程
<span class="math display">\[d x_t = \frac{1}{2} \nabla \log p(x) dt + dW_t\]</span>
其中<span class="math inline">\(w_t \sim \mathcal{N}(0, 1)\)</span>，那么<span class="math inline">\(x_t\)</span>随时间演化的稳态分布是<span class="math inline">\(p(x)\)</span>。因此，如果我们能够根据上面的随机微分方程来演化<span class="math inline">\(x_t\)</span>，那么经过足够长时间达到稳态后，我们就可以得到<span class="math inline">\(p(x)\)</span>的采样。这个方程在时间上是连续的，在实际采样中，我们还需要把上面的方程离散化。但是在离散化生成每一步的样本后，还需要一步Metropolis adjustment，不然有可能无法收敛到目标分布。</p>
<p>MALA的具体算法如下：</p>
<ol style="list-style-type: decimal">
<li>对于<span class="math inline">\(x_n\)</span>，计算目标分布对数的梯度<span class="math inline">\(\delta x_n = \nabla \log p(x_n)\)</span></li>
<li>生成新的proposal <span class="math inline">\(x_{n+1}&#39; \sim \mathcal{N}(x_n + \frac{\sigma^2}{2} \delta x_n, \sigma^2 \mathbf{1})\)</span> (<span class="math inline">\(\sigma\)</span>直观上是离散化之后的时间步长)</li>
<li>计算接受概率<span class="math inline">\(a(x_{n+1}&#39; | x_n) = \min (1, \frac{p(x_{n+1}&#39; Q(x_n | x_{n+1}&#39;))}{p(x_n) Q(x_{n+1}&#39; | x_n)})\)</span>。其中<span class="math inline">\(Q(x_{n+1}&#39; | x_n) = \mathcal{N}(x_n + \frac{\sigma^2}{2} \delta x_n, \sigma^2 \mathbf{1})\)</span>，<span class="math inline">\(Q(x_n | x_{n+1}&#39;) = \mathcal{N}(x_{n+1}&#39; + \frac{\sigma^2}{2} \delta x_{n+1}&#39;, \sigma^2 \mathbf{1})\)</span>(<span class="math inline">\(\delta x_{n+1}&#39; = \nabla \log p(x_{n+1}&#39;)\)</span>)。</li>
<li>以<span class="math inline">\(a(x_{n+1}&#39; | x_n)\)</span>接受新的样本<span class="math inline">\(x_{n+1}&#39;\)</span>，否则<span class="math inline">\(x_{n+1} = x_n\)</span></li>
</ol>
</div>
<div id="implementation" class="section level1">
<h1>Implementation</h1>
<p>我们考虑和之前MCMC方法里一样的问题。首先生成样本</p>
<pre class="python"><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from functools import partial
from scipy.stats import multivariate_normal

np.random.seed(1234)
sigma_y = 2
sample_size = 30
theta1 = np.random.randn()
theta2 = np.random.randn()

print(f&quot;Theta1: {theta1}&quot;)</code></pre>
<pre><code>## Theta1: 0.47143516373249306</code></pre>
<pre class="python"><code>print(f&quot;Theta2: {theta2}&quot;)</code></pre>
<pre><code>## Theta2: -1.1909756947064645</code></pre>
<pre class="python"><code>y_samples = theta1 + theta2**2 + sigma_y * np.random.randn(sample_size)
plt.plot(np.arange(sample_size), y_samples, &#39;o&#39;, alpha=0.5);
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>我们还是写出后验概率的函数</p>
<pre class="python"><code>def unnormalized_posterior(theta, y, sigma_y):
    theta1 = theta[0]
    theta2 = theta[1]
    return 100000*np.exp(-0.5*(theta1**2 + theta2**2)) * np.exp(-0.5*np.sum((y - theta1 - theta2**2)**2) / (sigma_y**2))
  
theta_contour_calc = partial(unnormalized_posterior, y=y_samples, sigma_y=sigma_y)</code></pre>
<p>那么后验概率分布如下</p>
<pre><code>## &lt;matplotlib.contour.QuadContourSet object at 0x7f9870716630&gt;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>由于MALA还需要对数概率分布的梯度，我们再定义对于上面问题的函数</p>
<pre class="python"><code>def grad_log_posterior(q):
    q1, q2 = q[0], q[1]
    dHdq1 = q1 - np.sum(y_samples - q1 - q2**2) / (sigma_y**2)
    dHdq2 = q2 - 2 * q2 * np.sum(y_samples - q1 - q2**2) / (sigma_y**2)
    return np.array([dHdq1, dHdq2])</code></pre>
<p>为了进行MALA采样，我们先定义一个后验概率的类。主要包含计算后验概率(<code>posterior</code>)和对数后验概率梯度(<code>grad_log_posterior</code>)的函数</p>
<pre class="python"><code>class PosteriorProb:
    def __init__(self, posterior_prob, grad_log_posterior):
        self.posterior_prob = posterior_prob
        self.grad_log_posterior = grad_log_posterior
        
    def posterior(self, theta):
        return self.posterior_prob(theta)
    
    def grad_log_poterior(self, theta):
        return self.grad_log_posterior(theta)</code></pre>
<p>下面我们可以定义采样类了</p>
<pre class="python"><code>class MALASampler:
    def __init__(self, posterior_prob, sigma=0.05):
        self.posterior_prob = posterior_prob
        self.sigma = sigma
        
    def sample_one_step(self, theta_init):
        grad_log_prob_init = self.posterior_prob.grad_log_posterior(theta_init)
        mean_theta_init = theta_init + 0.5 * self.sigma**2 * grad_log_prob_init
        noise_term = multivariate_normal.rvs(mean=np.zeros_like(theta_init), cov=self.sigma**2 * np.identity(theta_init.shape[0]))
        theta_proposal = mean_theta_init + noise_term
        grad_log_prob_proposal = self.posterior_prob.grad_log_posterior(theta_proposal)
        mean_theta_proposal = theta_proposal + 0.5 * self.sigma**2 * grad_log_prob_proposal
        current_tran_prob = np.exp(- 0.5 * np.sum((theta_proposal - mean_theta_init)**2) / (self.sigma**2))
        back_tran_prob = np.exp(-0.5 * np.sum((theta_init - mean_theta_proposal)**2) / (self.sigma**2))
        accept_rate = min(
            1, 
            (self.posterior_prob.posterior(theta_proposal) * back_tran_prob) / (
                self.posterior_prob.posterior(theta_init) * current_tran_prob
            )
        )
        rand_u = np.random.rand()
        if rand_u &lt; accept_rate:
            return theta_proposal, 1
        return theta_init, 0
        
    def sample(self, theta_init, num_samples):
        self.accept_hist = np.zeros(num_samples)
        state_shape = theta_init.shape[0]
        samples = np.zeros((num_samples+1, state_shape))
        for step in range(num_samples):
            theta_current, accept = self.sample_one_step(samples[step])
            samples[step+1] = theta_current
            self.accept_hist[step] = accept
        return samples</code></pre>
<p>现在我们可以生成样本了</p>
<pre class="python"><code>posterior = PosteriorProb(theta_contour_calc, grad_log_posterior)
mala_sampler = MALASampler(posterior, sigma=0.15)
samples = mala_sampler.sample(np.array([-0.5, 1.5]), 10000)</code></pre>
<p>采样结果如下
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>本文中我们简单介绍了一种加速MCMC采样方法收敛速度的算法MALA。这种算法的思想是利用目标分布的梯度信息来加速达到收敛的速度。后面文章中我们还会介绍别的加速MCMC收敛速度的方法。</p>
</div>
