---
title: 高斯混合模型7
author: Yi LIU
date: '2019-09-22'
slug: '7'
categories:
  - Work
tags:
  - Bayesian
  - Stats
  - Unsupervised learning
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="系列总结conclusion-of-the-series" class="section level1">
<h1>系列总结(Conclusion of the series)</h1>
<p>在本系列文章中，我们介绍了一个聚类算法中的常用模型：高斯混合模型。这个模型可以看作K-means的改进：相对于K-means的硬聚类，高斯混合模型是一种基于概率分布聚类模型。对于每一个观察值，我们可以计算它属于每一簇的后验概率<span class="math inline">\(p(z_{ik}|x_i)\)</span>。</p>
<p>这个模型可以从两种观点来求解。第一种是频率学派的<strong>最大似然函数</strong>的观点。这种观点下，合适的方法是引入<strong>隐变量</strong>，然后使用<strong>EM算法</strong>。</p>
<p>第二种是贝叶斯学派的<strong>最大后验概率</strong>观点。在这里，我们引入了高斯混合模型的<strong>生成模型</strong>，然后使用了两种不同的算法来近似求解后验概率：</p>
<ol style="list-style-type: decimal">
<li><strong>Monte-Carlo采样方法</strong>：这种方法是采用采样方法来近似后验概率。在采样数目足够大的情况下，采样得到的参数分布会收敛于真实后验概率。但是面对大量样本数据，以及高变量维度的情况下，scaling是一个问题。目前关于large-scale Monte Carlo methods已经有一些研究了，比如Hamiltonian Monte Carlo等。</li>
<li><strong>变分推断（variational inference）</strong>：这种方法是限定后验分布函数的函数空间，然后在这个受限的空间里寻找最大化ELBO的那个函数。这是一种决定性的近似方法，可以使用大规模的优化算法来求解large-scale问题。但是一个需要注意的是如何选取这个受限空间。</li>
</ol>
<p>读到这里了，一个很自然的问题是为什么我们要花这么大力气来介绍高斯混合模型，这样一个并不是特别复杂的模型。一个原因是通过这个模型，我们引入了上面的那些概念，而这些概念对于统计学来说都是很重要的。也就是说我们想要在这样一个简单的模型里面给大家展示一下统计学的多姿多彩。此外，在后面的文章里面，我们可能会进一步介绍上面引入的这些概念。</p>
<p>另外一个原因就是我之前有过一段工作经历是利用高斯混合模型来对肿瘤细胞的DNA变异情况进行表征，因此对于这个问题也有一些经验。所以在这里也算是我自己的一个总结。在这个过程中，我也学到了很多东西。比如我之前对于高斯混合模型的贝叶斯观点并不了解，也借这次机会增强了自己的了解。当然，这个了解很粗浅，特别是关于变分推断那个部分。如果有什么疏漏还请大家原谅。在RPub上有一个很好的<a href="http://rpubs.com/cakapourani/variational_bayes_gmm">教程</a>，大家可以参考。这个教程的作者是<a href="http://homepages.inf.ed.ac.uk/ckapoura/">Chantriolnt-Andreas Kapourani</a>，他的主页上还有很多高质量的教程。</p>
<p>高斯混合模型是一个很常用的模型，由于它具有一些比较好的性质，所以关于它的研究也很多。由于篇幅的限制，没有办法一一介绍。比如模型的选择，即<span class="math inline">\(K\)</span>的选择，我们这里并没有涉及。但是实际工作中肯定需要确定这个超参数的值。还有很多类似的话题，也只能留待读者诸君自己探索了。</p>
<p>关于本系列文章的代码，可以说是poor man’s implementation：代码质量惨不忍睹。但是我想要做的是通过简单的代码来解释文章的内容。如果后面有时间的话，我希望能够把文章内的代码写的更好一点。</p>
</div>
